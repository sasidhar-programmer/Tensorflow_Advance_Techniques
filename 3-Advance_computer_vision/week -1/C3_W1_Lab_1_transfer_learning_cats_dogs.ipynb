{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of C3_W1_Lab_1_transfer_learning_cats_dogs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYJqjq66JVQQ"
      },
      "source": [
        "# Basic transfer learning with cats and dogs data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oWuHhhcJVQQ"
      },
      "source": [
        "### Import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLbtB3uGKPX"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjfMJAHPJVQR"
      },
      "source": [
        "### Import modules and download the cats and dogs dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y23ucAFLoHop"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "data_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\"\n",
        "data_file_name = \"catsdogs.zip\"\n",
        "download_dir = '/tmp/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNVXCUNUJVQR"
      },
      "source": [
        "Check that the dataset has the expected number of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwMoZHxWOynx",
        "outputId": "1ffe2e3e-a55a-42a3-d858-07faac5c3ee3"
      },
      "source": [
        "print(\"Number of cat images:\",len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(\"Number of dog images:\", len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "\n",
        "# Expected Output:\n",
        "# Number of cat images: 12501\n",
        "# Number of dog images: 12501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of cat images: 12501\n",
            "Number of dog images: 12501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0riaptkJVQR"
      },
      "source": [
        "Create some folders that will store the training and test data.\n",
        "- There will be a training folder and a testing folder.\n",
        "- Each of these will have a subfolder for cats and another subfolder for dogs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qygIo4W5O1hQ"
      },
      "source": [
        "try:\n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZHD_c-sJVQR"
      },
      "source": [
        "### Split data into training and test sets\n",
        "\n",
        "- The following code put first checks if an image file is empty (zero length)\n",
        "- Of the files that are not empty, it puts 90% of the data into the training set, and 10% into the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M90EiIu0O314",
        "outputId": "4b2234fc-8667-41d5-cace-9fdadb97b41e"
      },
      "source": [
        "import random\n",
        "from shutil import copyfile\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    testing_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    testing_set = shuffled_set[:testing_length]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    for filename in testing_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TESTING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Expected output\n",
        "# 666.jpg is zero length, so ignoring\n",
        "# 11702.jpg is zero length, so ignoring"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "666.jpg is zero length, so ignoring.\n",
            "11702.jpg is zero length, so ignoring.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMx_pePuJVQR"
      },
      "source": [
        "Check that the training and test sets are the expected lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl8sQpM1O9xK",
        "outputId": "3a920f12-949a-4d51-9671-24d4126810e7"
      },
      "source": [
        "\n",
        "print(\"Number of training cat images\", len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
        "print(\"Number of training dog images\", len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
        "print(\"Number of testing cat images\", len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
        "print(\"Number of testing dog images\", len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "# expected output\n",
        "# Number of training cat images 11250\n",
        "# Number of training dog images 11250\n",
        "# Number of testing cat images 1250\n",
        "# Number of testing dog images 1250"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training cat images 11250\n",
            "Number of training dog images 11250\n",
            "Number of testing cat images 1250\n",
            "Number of testing dog images 1250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNz89__rJVQR"
      },
      "source": [
        "### Data augmentation (try adjusting the parameters)!\n",
        "\n",
        "Here, you'll use the `ImageDataGenerator` to perform data augmentation.  \n",
        "- Things like rotating and flipping the existing images allows you to generate training data that is more varied, and can help the model generalize better during training.  \n",
        "- You can also use the data generator to apply data augmentation to the validation set.\n",
        "\n",
        "You can use the default parameter values for a first pass through this lab.\n",
        "- Later, try to experiment with the parameters of `ImageDataGenerator` to improve the model's performance.\n",
        "- Try to drive reach 99.9% validation accuracy or better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVO1l8vAPE14",
        "outputId": "3fc1784d-d93f-4cde-bdfe-693ebc5ea914"
      },
      "source": [
        "\n",
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "# Experiment with your own parameters to reach 99.9% validation accuracy or better\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=100,\n",
        "                                                              class_mode='binary',\n",
        "                                                              target_size=(150, 150))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22498 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WchwDzWNJVQR"
      },
      "source": [
        "### Get and prepare the model\n",
        "\n",
        "You'll be using the `InceptionV3` model.  \n",
        "- Since you're making use of transfer learning, you'll load the pre-trained weights of the model.\n",
        "- You'll also freeze the existing layers so that they aren't trained on your downstream task with the cats and dogs data.\n",
        "- You'll also get a reference to the last layer, 'mixed7' because you'll add some layers after this last layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiPK1LlMOvm7",
        "outputId": "3bbabe75-3c1f-47fa-cb72-f8d400a05d97"
      },
      "source": [
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "weights_file = \"inception_v3.h5\"\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\n",
        "\n",
        "# Instantiate the model\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                include_top=False,\n",
        "                                weights=None)\n",
        "\n",
        "# load pre-trained weights\n",
        "pre_trained_model.load_weights(weights_file)\n",
        "\n",
        "# freeze the layers\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3edBz_IxJVQR"
      },
      "source": [
        "### Add layers\n",
        "Add some layers that you will train on the cats and dogs data.\n",
        "- `Flatten`: This will take the output of the `last_layer` and flatten it to a vector.\n",
        "- `Dense`: You'll add a dense layer with a relu activation.\n",
        "- `Dense`: After that, add a dense layer with a sigmoid activation.  The sigmoid will scale the output to range from 0 to 1, and allow you to interpret the output as a prediction between two categories (cats or dogs).\n",
        "\n",
        "Then create the model object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDidHXO1JVQR"
      },
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asCm8okXJVQR"
      },
      "source": [
        "### Train the model\n",
        "Compile the model, and then train it on the test data using `model.fit`\n",
        "- Feel free to adjust the number of epochs.  This project was originally designed with 20 epochs.\n",
        "- For the sake of time, you can use fewer epochs (2) to see how the code runs.\n",
        "- You can ignore the warnings about some of the images having corrupt EXIF data. Those will be skipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nxUncKWPRhR",
        "outputId": "4794fe87-a1b7-49d9-9857-8741ce54766a"
      },
      "source": [
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# train the model (adjust the number of epochs from 1 to improve performance)\n",
        "history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            epochs=2,\n",
        "            verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            " 42/225 [====>.........................] - ETA: 2:16 - loss: 0.4528 - acc: 0.8514"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 179s 795ms/step - loss: 0.2365 - acc: 0.9086 - val_loss: 0.0719 - val_acc: 0.9760\n",
            "Epoch 2/2\n",
            "225/225 [==============================] - 169s 753ms/step - loss: 0.1527 - acc: 0.9377 - val_loss: 0.0831 - val_acc: 0.9732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Oo6kM-JVQR"
      },
      "source": [
        "### Visualize the training and validation accuracy\n",
        "\n",
        "You can see how the training and validation accuracy change with each epoch on an x-y plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "erDopoQ5eNL7",
        "outputId": "647f7700-d4c2-4ae1-f439-148a362c7e15"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKc_1Qm8JVQR"
      },
      "source": [
        "### Predict on a test image\n",
        "\n",
        "You can upload any image and have the model predict whether it's a dog or a cat.\n",
        "- Find an image of a dog or cat\n",
        "- Run the following code cell.  It will ask you to upload an image.\n",
        "- The model will print \"is a dog\" or \"is a cat\" depending on the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0R9fsf4w29e"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}